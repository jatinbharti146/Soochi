{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import csv\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import signal\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id='AKIAYNHLFCQR7UYIG44O', \n",
    "        aws_secret_access_key='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'\n",
    "    )\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JATIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: use options instead of chrome_options\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103.105.77.48:8181', '91.135.194.22:56828', '46.232.132.249:53769', '186.46.128.90:56942', '197.242.206.64:34576', '195.146.133.142:61827', '103.86.155.78:52554', '36.90.69.239:55443', '219.85.34.248:80', '103.12.160.200:53281', '177.46.148.142:3128', '88.198.33.232:1080', '182.23.81.82:3128', '46.241.120.230:32412', '46.249.123.146:8080', '45.151.169.66:8080', '165.22.216.54:5836', '206.189.82.151:8080', '103.252.117.230:3128', '81.217.151.218:56193']\n"
     ]
    }
   ],
   "source": [
    "chromeOptions = Options()\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"--kiosk\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "driver = webdriver.Chrome(chrome_options=options)\n",
    "driver.get(\"https://sslproxies.org/\")\n",
    "driver.execute_script(\"return arguments[0].scrollIntoView(true);\", WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//table[@class='table table-striped table-bordered dataTable']//th[contains(., 'IP Address')]\"))))\n",
    "ips = [my_elem.get_attribute(\"innerHTML\") for my_elem in WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@class='table table-striped table-bordered dataTable']//tbody//tr[@role='row']/td[position() = 1]\")))]\n",
    "ports = [my_elem.get_attribute(\"innerHTML\") for my_elem in WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.XPATH, \"//table[@class='table table-striped table-bordered dataTable']//tbody//tr[@role='row']/td[position() = 2]\")))]\n",
    "driver.quit()\n",
    "proxies = []\n",
    "for i in range(0, len(ips)):\n",
    "    proxies.append(ips[i]+':'+ports[i])\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = ['51.91.254.196:31282','185.175.58.110:31282','202.5.17.61:31282','23.94.123.163:31282','200.106.55.125:80','96.62.169.138:31282','103.27.77.200:31282','185.193.113.52:31282','190.152.213.126:44523']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company(soup):\n",
    "    name = soup.select_one('span._2J4LW6')\n",
    "    if name!=None:\n",
    "        name = name.text\n",
    "    return name\n",
    "\n",
    "def get_title(soup):\n",
    "    name = soup.select_one('span._35KyD6')\n",
    "    if name!=None:\n",
    "        name = name.text\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_specification(soup):\n",
    "    temp = soup.select('div.row')\n",
    "    spec = {}\n",
    "    for each in temp:\n",
    "        key = each.select_one('div.col col-3-12 _1kyh2f')\n",
    "        value = each.select_one('div.col col-9-12 _1BMpvA')\n",
    "        if key!=None:\n",
    "            key=key.text\n",
    "        if value!=None:\n",
    "            value=value.text\n",
    "        spec[key]=value\n",
    "    return spec\n",
    "\n",
    "\n",
    "\n",
    "def get_price(driver):\n",
    "    prices = {}\n",
    "    sizes= driver.find_elements_by_css_selector('li._3hSwtk _2UBURg')\n",
    "    for each in sizes:\n",
    "        click=each.find_element_by_css_selector('a._1TJldG _2I_hq9 _1wX7_H')\n",
    "        click.click()\n",
    "        name = each.find_element_by_css_selector('a._1TJldG _2I_hq9 _2UBURg')\n",
    "        if name!=None:\n",
    "            name=name.text\n",
    "            name = re.sub('<.*?>', '', name)\n",
    "        price = driver.find_element_by_css_selector('div._1vC4OE _3qQ9m1')\n",
    "        if price!=None:\n",
    "            price=price.text\n",
    "            price = int(price.split(\" \")[1])\n",
    "        prices[name]=price\n",
    "    return prices\n",
    "\n",
    "def get_images(soup,unique_id):\n",
    "    links=[]\n",
    "    temp = [i.attrs.get('style') for i in soup.select('div._2_AcLJ _3_yGjX')]\n",
    "    var=1\n",
    "    for each in temp:\n",
    "        if each!=None:\n",
    "            link = re.findall('.*(http.*)\"',each)[0]\n",
    "            img=Image.open(urllib.request.urlopen(link))\n",
    "            name = unique_id + '_' + str(var) + '.jpg'\n",
    "            img.save(name)\n",
    "            upload_file(name,\"jatin\")\n",
    "            url_img = \"https://jatin.s3.ap-south-1.amazonaws.com/\" + name\n",
    "            links.append(url_img)\n",
    "            if os.path.exists(name):\n",
    "                os.remove(name)\n",
    "                print('file deleted :',name)\n",
    "            else:\n",
    "                print(\"The file does not exist\")\n",
    "            var=var+1\n",
    "    return links\n",
    "\n",
    "def get_seller(soup):\n",
    "    sold_by = soup.select_one('div._3HGjxn')\n",
    "    seller={}\n",
    "    if(sold_by!=None):\n",
    "        sold_by=sold_by.text\n",
    "    seller['sold by']=sold_by\n",
    "    temp = soup.select('span._3aS5mM')\n",
    "    if temp != None:\n",
    "        temp = temp.text\n",
    "    seller['Details']=temp\n",
    "    return seller\n",
    "\n",
    "\n",
    "def get_rat(soup):\n",
    "    rew = {}\n",
    "    rat = soup.select_one('div.hGSR34 bqXGTW')\n",
    "    raw = soup.select_one('span._38sUEc>span')\n",
    "    if rat!=None:\n",
    "        rat=rat.text\n",
    "    if raw!= None:\n",
    "        raw=raw.text\n",
    "    \n",
    "    rew[rat] = raw \n",
    "    return rew\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy selected: 51.91.254.196:31282\n",
      "Proxy selected: 185.175.58.110:31282\n",
      "Proxy selected: 202.5.17.61:31282\n",
      "Proxy selected: 23.94.123.163:31282\n",
      "Proxy selected: 200.106.55.125:80\n",
      "Proxy selected: 96.62.169.138:31282\n",
      "Proxy selected: 103.27.77.200:31282\n",
      "Proxy selected: 185.193.113.52:31282\n",
      "Proxy selected: 190.152.213.126:44523\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(proxies)):\n",
    "    try:\n",
    "        print(\"Proxy selected: {}\".format(proxies[i]))\n",
    "        chromeOptions = Options()\n",
    "        chromeOptions.add_argument(\"--headless\")\n",
    "        \n",
    "        #chromeOptions.add_argument('--proxy-server={}'.format(proxies[i]))\n",
    "        \n",
    "        driver = webdriver.Chrome(options=chromeOptions)\n",
    "        driver.get(\"https://www.flipkart.com/\")\n",
    "        page_source = driver.page_source\n",
    "        temp = BeautifulSoup(page_source,'lxml')\n",
    "        category_link = [i.attrs.get('href') for i in temp.select('a.desktop-categoryLink')]\n",
    "        \n",
    "        #opening each category\n",
    "        \n",
    "        geek=1\n",
    "        for each_cat_link in category_link:\n",
    "            each_cat_link = \"https://www.flipkart.com/\" + each_cat_link\n",
    "            new_driver = webdriver.Chrome(options=chromeOptions)\n",
    "            new_driver.get(each_cat_link)\n",
    "            \n",
    "            #pagination while loop\n",
    "            \n",
    "            while 1:\n",
    "                time.sleep(25)\n",
    "                page2_source = new_driver.page_source\n",
    "                temp2 = BeautifulSoup(page2_source,'lxml')\n",
    "                product_link = [i.attrs.get('href') for i in temp2.select('li.product-base>a')]\n",
    "            \n",
    "                #opening each product\n",
    "                rows=[]\n",
    "                for each_product_link in product_link:\n",
    "                    if geek==50:\n",
    "                        time.sleep(120)\n",
    "                        i=i+1\n",
    "                        if i==len(proxies):\n",
    "                            i=0\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            #chromeOptions.add_argument('--proxy-server={}'.format(proxies[i]))\n",
    "                            prd_driver = webdriver.Chrome(options=chromeOptions)\n",
    "                            each_product_link = \"https://www.flipkart.com/\" + each_product_link\n",
    "                            prd_driver.get(each_product_link)\n",
    "                            break\n",
    "                        except:\n",
    "                            i=i+1\n",
    "                            if i==len(proxies):\n",
    "                                i=0\n",
    "                            print('proxy error or page error at each product')\n",
    "                    time.sleep(10)\n",
    "                    \n",
    "                     #click on sizes and get price\n",
    "                        \n",
    "                    prices = {}\n",
    "                    size_button = prd_driver.find_elements_by_css_selector('a._1TJldG _2I_hq9 _1wX7_H')\n",
    "                    size_name= prd_driver.find_elements_by_css_selector('a._1TJldG _2I_hq9 _2UBURg')\n",
    "                    print(\"\\n sizes :\",len(size_button),\"    \",len(size_name),'\\n')\n",
    "                    count=0\n",
    "                    for each_click in size_button:\n",
    "                        try:\n",
    "                            each_click.click()\n",
    "                            time.sleep(2)\n",
    "                        except:\n",
    "                            print('click error')\n",
    "                            count=count+1\n",
    "                            continue\n",
    "                        time.sleep(2)\n",
    "                        name=size_name[count]\n",
    "                        if name!=None:\n",
    "                            name=name.text\n",
    "                            name = re.sub('<.*?>', '', name)\n",
    "                        price = prd_driver.find_element_by_css_selector('div._1vC4OE _3qQ9m1')\n",
    "                        if price!=None:\n",
    "                            price=price.text\n",
    "                            price = int(price.split(\" \")[1])\n",
    "                        prices[name]=price\n",
    "                        count=count+1\n",
    "                    print(prices)\n",
    "                    \n",
    "                    #click on + -->to get product details\n",
    "    \n",
    "                    see_det = prd_driver.find_element_by_css_selector('div.col col-1-12 _3rsYI2>img')\n",
    "                    try:\n",
    "                        see_det.click()\n",
    "                        time.sleep(2)\n",
    "                    except:\n",
    "                        print('see more click error')\n",
    "                        \n",
    "                     #click on see more -->to get more product details\n",
    "    \n",
    "                    see_more = prd_driver.find_element_by_css_selector('button._2AkmmA _1jdA3N')\n",
    "                    try:\n",
    "                        see_more.click()\n",
    "                        time.sleep(2)\n",
    "                    except:\n",
    "                        print('see more click error')\n",
    "                    \n",
    "                    #click on view supplier information -->to get supplier info\n",
    "                    \n",
    "                    view_supp = prd_driver.find_element_by_css_selector('div._3HGjxn>span')\n",
    "                    try:\n",
    "                        view_supp.click()\n",
    "                        time.sleep(2)\n",
    "                    except:\n",
    "                        print('view supplier information click error')\n",
    "                    \n",
    "                    \n",
    "                    page3_source = prd_driver.page_source\n",
    "                    soup = BeautifulSoup(page3_source,'lxml')               \n",
    "                    \n",
    "                    company = get_company(soup)\n",
    "                    print('\\n company:',company)\n",
    "                    \n",
    "                    title = get_title(soup)\n",
    "                    print('\\n title:',title)\n",
    "                    \n",
    "                    specification = get_specification(soup)\n",
    "                    print('\\n spec:',specification)\n",
    "                    \n",
    "                   \n",
    "                    seller = get_seller(soup)\n",
    "                    print('\\n seller:',seller)\n",
    "                   \n",
    "                    images = get_images(soup,unique_id)\n",
    "                    print(images)\n",
    "                    \n",
    "                    #storing in dict\n",
    "                    \n",
    "                    extras=dict(Company=company,Product=title,Description=description,Specification=specification)\n",
    "                    row = dict(Id=unique_id,Images=images,Prices=prices,Detail=extras,Supplier=seller)\n",
    "                    rows.append(row)\n",
    "                    geek=geek+1\n",
    "                    \n",
    "                #go to next page \n",
    "                try:\n",
    "                    pivot=0\n",
    "                    while 1:\n",
    "                        next_click = new_driver.find_element_by_css_selector('a._3fVaIS>span')\n",
    "                        if next_click ==None:\n",
    "                            pivot=1\n",
    "                            break\n",
    "                        try:\n",
    "                            next_click.click()\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if pivot==1:\n",
    "                        break\n",
    "                    print('yup')\n",
    "                    if os.path.exists(\"flipkart_data.csv\"):\n",
    "                        with open (\"flipkart_data.csv\",\"a+\") as f:\n",
    "                            writer = csv.DictWriter(f,fieldnames=row.keys())\n",
    "                            writer.writerows(rows)\n",
    "                            upload_file('flipkart_data.csv',\"jatin\")\n",
    "                    else: \n",
    "                        with open (\"flipkart_data.csv\",\"a+\") as f:\n",
    "                            writer = csv.DictWriter(f,fieldnames=row.keys())\n",
    "                            writer.writeheader()\n",
    "                            writer.writerows(rows)\n",
    "                            upload_file('flipkart_data.csv',\"jatin\")\n",
    "                except:\n",
    "                    print('no next page or next click error')\n",
    "                    break    \n",
    "    except:\n",
    "        print('proxyerror or page error at home page')\n",
    "        continue;\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
